{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efe7a5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Airbnb Smart Pricing Engine Training\n",
      "============================================================\n",
      "üìÅ Project root: /Users/adityapandey/My Files/Thesis Sri Ganesh/Data Set/7\n",
      "üìä Data directory: /Users/adityapandey/My Files/Thesis Sri Ganesh/Data Set/7/data\n",
      "ü§ñ Models directory: /Users/adityapandey/My Files/Thesis Sri Ganesh/Data Set/7/models\n",
      "üì• Loading data...\n",
      "‚úÖ Loaded 6481 listings and 293744 reviews\n",
      "‚úÖ After merging: 6481 listings with review data\n",
      "üîß Engineering features...\n",
      "‚úÖ Feature engineering complete. Shape: (5045, 32)\n",
      "ü§ñ Training ensemble models...\n",
      "‚úÖ Trained ExtraTreesUltra\n",
      "‚úÖ Trained GradientBoostingUltra\n",
      "‚úÖ Trained RandomForestUltra\n",
      "‚úÖ Tabular ensemble trained. R¬≤ = 0.857, MAE = $28.45\n",
      "üîó Training multimodal model...\n",
      "‚úÖ Multimodal model trained. R¬≤ = 0.864, MAE = $26.91\n",
      "üöÄ Improvement: R¬≤ +0.7%, MAE +5.4%\n",
      "üíæ Saving models...\n",
      "üìÑ Creating JSON models for Streamlit...\n",
      "‚úÖ JSON models created successfully!\n",
      "\n",
      "============================================================\n",
      "üéâ TRAINING COMPLETE!\n",
      "============================================================\n",
      "üìä TABULAR MODEL PERFORMANCE\n",
      "   R¬≤ Score: 0.857 (85.7% accuracy)\n",
      "   Cross-Validation: 0.851 (¬±0.020)\n",
      "   MAE: $28.45\n",
      "\n",
      "üîó MULTIMODAL MODEL PERFORMANCE\n",
      "   R¬≤ Score: 0.864 (86.4% accuracy)\n",
      "   MAE: $26.91\n",
      "\n",
      "üöÄ IMPROVEMENT\n",
      "   R¬≤ Improvement: +0.7%\n",
      "   MAE Improvement: +5.4%\n",
      "\n",
      "üìÅ FILES CREATED:\n",
      "   ‚úÖ tabular_model_clean.pkl\n",
      "   ‚úÖ multimodal_model_clean.pkl\n",
      "   ‚úÖ preprocessor_clean.pkl\n",
      "   ‚úÖ metadata_clean.pkl\n",
      "   ‚úÖ streamlit_simple_model.json\n",
      "   ‚úÖ streamlit_linear_model.json\n",
      "   ‚úÖ streamlit_complete_model.json\n",
      "   ‚úÖ model_data_for_streamlit.json\n",
      "   ‚úÖ model_state.json\n",
      "   ‚úÖ preprocessor_simple.pkl\n",
      "============================================================\n",
      "üéØ Ready for Streamlit deployment!\n"
     ]
    }
   ],
   "source": [
    "# Airbnb Smart Pricing Engine - Complete Training Pipeline\n",
    "# Combines data processing, model training, and export for Streamlit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PowerTransformer, QuantileTransformer, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.stats import skew\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "# Text processing imports\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import shap\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"üöÄ Starting Airbnb Smart Pricing Engine Training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SETUP PATHS AND DIRECTORIES\n",
    "# ==============================================================================\n",
    "\n",
    "# Set up paths for organized project structure\n",
    "project_root = os.path.dirname(os.getcwd())  # Go up one level from notebooks/\n",
    "data_dir = os.path.join(project_root, \"data\")\n",
    "models_dir = os.path.join(project_root, \"models\")\n",
    "\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üìä Data directory: {data_dir}\")\n",
    "print(f\"ü§ñ Models directory: {models_dir}\")\n",
    "\n",
    "# Ensure models directory exists\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs('model_artifacts', exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. UTILITY CLASSES AND FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def get_sentiment_features(texts, max_features=100):\n",
    "    \"\"\"Extract sentiment and text features using TF-IDF\"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2\n",
    "    )\n",
    "    \n",
    "    cleaned_texts = [clean_text(text) for text in texts]\n",
    "    tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n",
    "    svd = TruncatedSVD(n_components=min(20, max_features))\n",
    "    reduced_features = svd.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    return reduced_features, vectorizer, svd\n",
    "\n",
    "class DistilBertTextEncoder:\n",
    "    \"\"\"Text encoder using DistilBERT for review processing\"\"\"\n",
    "    def __init__(self, max_length=256, batch_size=8):\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.model.eval()\n",
    "    \n",
    "    def encode_texts(self, texts):\n",
    "        \"\"\"Encode texts to embeddings\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_texts = texts[i:i + self.batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # CLS token\n",
    "            \n",
    "            all_embeddings.append(embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "\n",
    "class ExplainableMultimodalRegressor:\n",
    "    \"\"\"Multimodal regressor combining tabular and text data with explanations\"\"\"\n",
    "    def __init__(self, tabular_model, text_encoder, meta_model):\n",
    "        self.tabular_model = tabular_model\n",
    "        self.text_encoder = text_encoder\n",
    "        self.meta_model = meta_model\n",
    "        self.explainer = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def fit(self, X_tabular, X_text, y):\n",
    "        \"\"\"Fit the multimodal model\"\"\"\n",
    "        # Fit tabular model\n",
    "        self.tabular_model.fit(X_tabular, y)\n",
    "        self.feature_names = X_tabular.columns.tolist()\n",
    "        \n",
    "        # Get tabular predictions\n",
    "        tabular_preds = self.tabular_model.predict(X_tabular)\n",
    "        \n",
    "        # Encode text\n",
    "        text_embeddings = self.text_encoder.encode_texts(X_text)\n",
    "        \n",
    "        # Combine features for meta-learner\n",
    "        combined_features = np.column_stack([\n",
    "            tabular_preds.reshape(-1, 1),\n",
    "            text_embeddings\n",
    "        ])\n",
    "        \n",
    "        # Fit meta-model\n",
    "        self.meta_model.fit(combined_features, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_tabular, X_text):\n",
    "        \"\"\"Make predictions using both tabular and text data\"\"\"\n",
    "        # Get tabular predictions\n",
    "        tabular_preds = self.tabular_model.predict(X_tabular)\n",
    "        \n",
    "        # Encode text\n",
    "        text_embeddings = self.text_encoder.encode_texts(X_text)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = np.column_stack([\n",
    "            tabular_preds.reshape(-1, 1),\n",
    "            text_embeddings\n",
    "        ])\n",
    "        \n",
    "        # Meta-model prediction\n",
    "        return self.meta_model.predict(combined_features)\n",
    "    \n",
    "    def score(self, X_tabular, X_text, y):\n",
    "        \"\"\"Calculate R¬≤ score\"\"\"\n",
    "        predictions = self.predict(X_tabular, X_text)\n",
    "        return r2_score(y, predictions)\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Get feature importance from tabular model\"\"\"\n",
    "        if hasattr(self.tabular_model, 'feature_importances_'):\n",
    "            return self.tabular_model.feature_importances_\n",
    "        return None\n",
    "    \n",
    "    def explain_prediction(self, X_single, text_single):\n",
    "        \"\"\"Explain a single prediction\"\"\"\n",
    "        explanations = {}\n",
    "        \n",
    "        try:\n",
    "            # Convert Series to DataFrame if needed\n",
    "            if isinstance(X_single, pd.Series):\n",
    "                X_df = X_single.to_frame().T\n",
    "            else:\n",
    "                X_df = X_single if isinstance(X_single, pd.DataFrame) else pd.DataFrame([X_single], columns=self.feature_names)\n",
    "            \n",
    "            # Get predictions\n",
    "            tabular_pred = self.tabular_model.predict(X_df)[0]\n",
    "            final_pred = self.predict(X_df, [text_single])[0]\n",
    "            \n",
    "            explanations['predictions'] = {\n",
    "                'tabular_prediction': float(tabular_pred),\n",
    "                'final_prediction': float(final_pred),\n",
    "                'text_contribution': float(final_pred - tabular_pred)\n",
    "            }\n",
    "            \n",
    "            # Feature importance fallback if no SHAP\n",
    "            if self.explainer is None:\n",
    "                feature_importance = self.get_feature_importance()\n",
    "                if feature_importance is not None and self.feature_names:\n",
    "                    explanations['tabular'] = dict(zip(self.feature_names, feature_importance))\n",
    "                else:\n",
    "                    explanations['tabular'] = {}\n",
    "            else:\n",
    "                # Use SHAP if available\n",
    "                shap_values = self.explainer(X_df)\n",
    "                if hasattr(shap_values, 'values'):\n",
    "                    feature_importance = shap_values.values[0]\n",
    "                else:\n",
    "                    feature_importance = shap_values[0]\n",
    "                explanations['tabular'] = dict(zip(self.feature_names, feature_importance))\n",
    "            \n",
    "        except Exception as e:\n",
    "            explanations['error'] = str(e)\n",
    "            explanations['tabular'] = {}\n",
    "            explanations['predictions'] = {}\n",
    "        \n",
    "        return explanations\n",
    "\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"Convert numpy types to Python native types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_json_serializable(v) for v in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DATA LOADING AND PREPROCESSING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üì• Loading data...\")\n",
    "\n",
    "# Load data from organized structure\n",
    "listings_path = os.path.join(data_dir, 'listings.csv')\n",
    "reviews_path = os.path.join(data_dir, 'reviews.csv')\n",
    "\n",
    "df = pd.read_csv(listings_path)\n",
    "reviews_df = pd.read_csv(reviews_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} listings and {len(reviews_df)} reviews\")\n",
    "\n",
    "# Aggregate review data by listing_id\n",
    "review_aggregated = reviews_df.groupby('listing_id').agg({\n",
    "    'comments': lambda x: ' '.join(x.dropna().astype(str)) if len(x.dropna()) > 0 else '',\n",
    "    'id': 'count'\n",
    "}).reset_index()\n",
    "review_aggregated.columns = ['id', 'combined_reviews', 'review_count']\n",
    "\n",
    "# Merge with listings data\n",
    "df = df.merge(review_aggregated, on='id', how='left')\n",
    "df['combined_reviews'] = df['combined_reviews'].fillna('')\n",
    "df['review_count'] = df['review_count'].fillna(0)\n",
    "\n",
    "print(f\"‚úÖ After merging: {len(df)} listings with review data\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîß Engineering features...\")\n",
    "\n",
    "# Clean price data\n",
    "df['price_clean'] = df['price'].replace(r'[\\$,]', '', regex=True)\n",
    "df['price_clean'] = pd.to_numeric(df['price_clean'], errors='coerce')\n",
    "df = df.dropna(subset=['price_clean'])\n",
    "\n",
    "# Remove outliers\n",
    "Q1 = df['price_clean'].quantile(0.25)\n",
    "Q3 = df['price_clean'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 3 * IQR\n",
    "df = df[(df['price_clean'] >= lower_bound) & (df['price_clean'] <= upper_bound)]\n",
    "\n",
    "# Apply log transformation if skewed\n",
    "y_skewness = skew(df['price_clean'].dropna())\n",
    "if abs(y_skewness) > 1:\n",
    "    df['price_clean'] = np.log1p(df['price_clean'])\n",
    "\n",
    "# Create derived features\n",
    "if 'accommodates' in df.columns:\n",
    "    df['price_per_person'] = df['price_clean'] / df['accommodates'].replace(0, 1)\n",
    "\n",
    "if 'bedrooms' in df.columns and 'beds' in df.columns:\n",
    "    df['beds_per_bedroom'] = df['beds'] / df['bedrooms'].replace(0, 1)\n",
    "\n",
    "if 'bathrooms_text' in df.columns:\n",
    "    df['bathrooms_numeric'] = df['bathrooms_text'].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "\n",
    "if 'neighbourhood_cleansed' in df.columns:\n",
    "    neighbourhood_counts = df['neighbourhood_cleansed'].value_counts()\n",
    "    df['neighbourhood_popularity'] = df['neighbourhood_cleansed'].map(neighbourhood_counts)\n",
    "\n",
    "if 'host_is_superhost' in df.columns:\n",
    "    df['is_superhost_numeric'] = (df['host_is_superhost'] == 't').astype(int)\n",
    "\n",
    "if 'amenities' in df.columns:\n",
    "    df['amenities_count'] = df['amenities'].str.count(',') + 1\n",
    "    df['amenities_count'] = df['amenities_count'].fillna(0)\n",
    "    \n",
    "    # Key amenities\n",
    "    key_amenities = ['wifi', 'kitchen', 'parking', 'pool']\n",
    "    for amenity in key_amenities:\n",
    "        df[f'has_{amenity}'] = df['amenities'].str.lower().str.contains(amenity, na=False).astype(int)\n",
    "\n",
    "if 'availability_365' in df.columns:\n",
    "    df['availability_rate'] = df['availability_365'] / 365\n",
    "\n",
    "# Define feature sets\n",
    "numerical_features = [\n",
    "    'accommodates', 'bedrooms', 'beds', 'bathrooms_numeric',\n",
    "    'price_per_person', 'beds_per_bedroom', 'neighbourhood_popularity',\n",
    "    'is_superhost_numeric', 'amenities_count', 'minimum_nights', 'maximum_nights',\n",
    "    'availability_365', 'availability_rate', 'number_of_reviews', 'review_scores_rating',\n",
    "    'calculated_host_listings_count'\n",
    "] + [f'has_{amenity}' for amenity in key_amenities]\n",
    "\n",
    "categorical_features = ['neighbourhood_cleansed', 'room_type', 'property_type']\n",
    "\n",
    "# Filter existing columns\n",
    "numerical_features = [col for col in numerical_features if col in df.columns]\n",
    "categorical_features = [col for col in categorical_features if col in df.columns]\n",
    "\n",
    "# Create feature matrix\n",
    "X = df[numerical_features + categorical_features].copy()\n",
    "y = df['price_clean'].copy()\n",
    "\n",
    "# Additional engineered features\n",
    "if 'accommodates' in X.columns and 'bedrooms' in X.columns:\n",
    "    X['space_ratio'] = X['accommodates'] / (X['bedrooms'].replace(0, 1))\n",
    "    X['space_efficiency'] = X['accommodates'] / (X['bedrooms'].replace(0, 1) + 1)\n",
    "\n",
    "if 'number_of_reviews' in X.columns and 'review_scores_rating' in X.columns:\n",
    "    X['review_velocity'] = X['number_of_reviews'] / 100\n",
    "    X['review_quality_weighted'] = X['number_of_reviews'] * X['review_scores_rating'] / 100\n",
    "\n",
    "if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "    city_lat, city_lon = df['latitude'].median(), df['longitude'].median()\n",
    "    X['distance_from_center'] = np.sqrt((df['latitude'] - city_lat)**2 + (df['longitude'] - city_lon)**2)\n",
    "\n",
    "if 'name' in df.columns:\n",
    "    X['name_length'] = df['name'].str.len().fillna(0)\n",
    "    luxury_words = ['luxury', 'deluxe', 'premium', 'exclusive', 'elegant']\n",
    "    X['has_luxury_words'] = df['name'].str.lower().str.contains('|'.join(luxury_words), na=False).astype(int)\n",
    "\n",
    "if 'amenities' in df.columns:\n",
    "    premium_amenities = ['pool', 'hot tub', 'gym', 'elevator', 'doorman', 'concierge']\n",
    "    X['premium_amenities_count'] = sum(df['amenities'].str.lower().str.contains(amenity, na=False).astype(int) for amenity in premium_amenities)\n",
    "\n",
    "if 'host_since' in df.columns:\n",
    "    df['host_since'] = pd.to_datetime(df['host_since'], errors='coerce')\n",
    "    X['host_experience_years'] = (pd.Timestamp.now() - df['host_since']).dt.days / 365\n",
    "    X['host_experience_years'] = X['host_experience_years'].fillna(0)\n",
    "\n",
    "# Handle missing values\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if X[col].isnull().any():\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if X[col].isnull().any():\n",
    "        X[col] = X[col].fillna('Unknown')\n",
    "\n",
    "# Reset indices\n",
    "df = df.reset_index(drop=True)\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Feature engineering complete. Shape: {X.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TRAIN-TEST SPLIT AND NEIGHBORHOOD ENCODING\n",
    "# ==============================================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Add neighborhood-based features\n",
    "if 'neighbourhood_cleansed' in X_train.columns:\n",
    "    train_with_target = X_train.copy()\n",
    "    train_with_target['target'] = y_train\n",
    "    neighborhood_stats = train_with_target.groupby('neighbourhood_cleansed')['target'].agg(['mean', 'std']).reset_index()\n",
    "    neighborhood_stats.columns = ['neighbourhood_cleansed', 'neighborhood_avg_price', 'neighborhood_price_std']\n",
    "    \n",
    "    X_train = X_train.merge(neighborhood_stats, on='neighbourhood_cleansed', how='left')\n",
    "    X_test = X_test.merge(neighborhood_stats, on='neighbourhood_cleansed', how='left')\n",
    "    \n",
    "    overall_median = neighborhood_stats['neighborhood_avg_price'].median()\n",
    "    overall_std = neighborhood_stats['neighborhood_price_std'].median()\n",
    "    \n",
    "    X_train['neighborhood_avg_price'] = X_train['neighborhood_avg_price'].fillna(overall_median)\n",
    "    X_train['neighborhood_price_std'] = X_train['neighborhood_price_std'].fillna(overall_std)\n",
    "    X_test['neighborhood_avg_price'] = X_test['neighborhood_avg_price'].fillna(overall_median)\n",
    "    X_test['neighborhood_price_std'] = X_test['neighborhood_price_std'].fillna(overall_std)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. PREPROCESSING PIPELINE  \n",
    "# ==============================================================================\n",
    "\n",
    "numerical_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson')),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=min(len(X_train), 500), random_state=42))\n",
    "        ]), numerical_cols.tolist()),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'), categorical_cols.tolist())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. MODEL TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"ü§ñ Training ensemble models...\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'ExtraTreesUltra': ExtraTreesRegressor(\n",
    "        n_estimators=500, max_depth=25, min_samples_split=2, \n",
    "        min_samples_leaf=1, max_features='sqrt', bootstrap=True, \n",
    "        oob_score=True, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'GradientBoostingUltra': GradientBoostingRegressor(\n",
    "        n_estimators=500, learning_rate=0.05, max_depth=7, \n",
    "        min_samples_split=10, min_samples_leaf=4, subsample=0.8, \n",
    "        max_features='sqrt', random_state=42\n",
    "    ),\n",
    "    'RandomForestUltra': RandomForestRegressor(\n",
    "        n_estimators=500, max_depth=30, min_samples_split=5, \n",
    "        min_samples_leaf=2, max_features='sqrt', bootstrap=True, \n",
    "        oob_score=True, random_state=42, n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train individual models\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([('preprocessor', preprocessor), ('regressor', model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    trained_models[name] = pipeline\n",
    "    print(f\"‚úÖ Trained {name}\")\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = VotingRegressor(estimators=[(name, model) for name, model in trained_models.items()], n_jobs=-1)\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate tabular model\n",
    "test_score = ensemble.score(X_test, y_test)\n",
    "y_pred = ensemble.predict(X_test)\n",
    "cv_scores = cross_val_score(ensemble, X_train, y_train, cv=8, scoring='r2', n_jobs=-1)\n",
    "\n",
    "if abs(y_skewness) > 1:\n",
    "    actual_prices = np.expm1(y_test)\n",
    "    predicted_prices = np.expm1(y_pred)\n",
    "else:\n",
    "    actual_prices = y_test\n",
    "    predicted_prices = y_pred\n",
    "\n",
    "mae = mean_absolute_error(actual_prices, predicted_prices)\n",
    "\n",
    "print(f\"‚úÖ Tabular ensemble trained. R¬≤ = {test_score:.3f}, MAE = ${mae:.2f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. MULTIMODAL MODEL TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîó Training multimodal model...\")\n",
    "\n",
    "# Get text data\n",
    "text_data_train = df.loc[X_train.index, 'combined_reviews'].tolist()\n",
    "text_data_test = df.loc[X_test.index, 'combined_reviews'].tolist()\n",
    "\n",
    "# Create components\n",
    "text_encoder = DistilBertTextEncoder(max_length=256, batch_size=8)\n",
    "meta_learner = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create multimodal model\n",
    "multimodal_model = ExplainableMultimodalRegressor(\n",
    "    tabular_model=ensemble,\n",
    "    text_encoder=text_encoder,\n",
    "    meta_model=meta_learner\n",
    ")\n",
    "\n",
    "# Fit multimodal model\n",
    "multimodal_model.fit(X_train, text_data_train, y_train)\n",
    "\n",
    "# Evaluate multimodal model\n",
    "multimodal_score = multimodal_model.score(X_test, text_data_test, y_test)\n",
    "multimodal_pred = multimodal_model.predict(X_test, text_data_test)\n",
    "\n",
    "if abs(y_skewness) > 1:\n",
    "    multimodal_actual_prices = np.expm1(y_test)\n",
    "    multimodal_predicted_prices = np.expm1(multimodal_pred)\n",
    "else:\n",
    "    multimodal_actual_prices = y_test\n",
    "    multimodal_predicted_prices = multimodal_pred\n",
    "\n",
    "multimodal_mae = mean_absolute_error(multimodal_actual_prices, multimodal_predicted_prices)\n",
    "\n",
    "improvement = ((multimodal_score - test_score) / test_score) * 100\n",
    "mae_improvement = ((mae - multimodal_mae) / mae) * 100\n",
    "\n",
    "print(f\"‚úÖ Multimodal model trained. R¬≤ = {multimodal_score:.3f}, MAE = ${multimodal_mae:.2f}\")\n",
    "print(f\"üöÄ Improvement: R¬≤ +{improvement:.1f}%, MAE +{mae_improvement:.1f}%\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. MODEL EXPORT AND SAVING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üíæ Saving models...\")\n",
    "\n",
    "# Save complex models for backup\n",
    "joblib.dump(ensemble, 'model_artifacts/tabular_model.joblib')\n",
    "joblib.dump(multimodal_model, 'model_artifacts/multimodal_model.joblib')\n",
    "joblib.dump(preprocessor, 'model_artifacts/preprocessor.joblib')\n",
    "\n",
    "# Save clean models for Streamlit\n",
    "def clean_model_for_export(model):\n",
    "    \"\"\"Remove problematic numpy random states\"\"\"\n",
    "    from copy import deepcopy\n",
    "    model_copy = deepcopy(model)\n",
    "    \n",
    "    if hasattr(model_copy, 'random_state'):\n",
    "        model_copy.random_state = 42\n",
    "    \n",
    "    if hasattr(model_copy, 'estimators_'):\n",
    "        for estimator in model_copy.estimators_:\n",
    "            if hasattr(estimator, 'random_state'):\n",
    "                estimator.random_state = 42\n",
    "                \n",
    "    if hasattr(model_copy, 'named_steps'):\n",
    "        for step_name, step in model_copy.named_steps.items():\n",
    "            if hasattr(step, 'random_state'):\n",
    "                step.random_state = 42\n",
    "                \n",
    "    return model_copy\n",
    "\n",
    "# Clean and save models\n",
    "clean_tabular = clean_model_for_export(ensemble)\n",
    "clean_multimodal = clean_model_for_export(multimodal_model)\n",
    "clean_multimodal.explainer = None  # Remove explainer for compatibility\n",
    "\n",
    "with open('tabular_model_clean.pkl', 'wb') as f:\n",
    "    pickle.dump(clean_tabular, f, protocol=4)\n",
    "\n",
    "with open('multimodal_model_clean.pkl', 'wb') as f:\n",
    "    pickle.dump(clean_multimodal, f, protocol=4)\n",
    "\n",
    "with open('preprocessor_clean.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f, protocol=4)\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'categorical_features': categorical_features,\n",
    "    'numerical_features': numerical_cols.tolist(),\n",
    "    'y_skewness': y_skewness,\n",
    "    'price_stats': {\n",
    "        'mean': df['price_clean'].mean(),\n",
    "        'std': df['price_clean'].std(),\n",
    "        'min': df['price_clean'].min(),\n",
    "        'max': df['price_clean'].max()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('metadata_clean.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f, protocol=4)\n",
    "\n",
    "# ==============================================================================\n",
    "# 10. CREATE JSON MODELS FOR STREAMLIT\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üìÑ Creating JSON models for Streamlit...\")\n",
    "\n",
    "# Prepare clean numerical data\n",
    "X_clean = X_train.copy()\n",
    "y_clean = y_train.copy()\n",
    "\n",
    "# Get only numerical features to avoid categorical encoding issues\n",
    "numerical_features_only = X_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "X_numerical = X_clean[numerical_features_only].copy()\n",
    "X_numerical = X_numerical.fillna(X_numerical.median())\n",
    "\n",
    "# Train simple models for JSON export\n",
    "simple_rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=1)\n",
    "simple_lr = LinearRegression()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_numerical)\n",
    "simple_rf.fit(X_numerical, y_clean)\n",
    "simple_lr.fit(X_scaled, y_clean)\n",
    "\n",
    "# Test performance\n",
    "X_test_numerical = X_test[numerical_features_only].fillna(X_test[numerical_features_only].median())\n",
    "X_test_scaled = scaler.transform(X_test_numerical)\n",
    "\n",
    "rf_score = simple_rf.score(X_test_numerical, y_test)\n",
    "lr_score = simple_lr.score(X_test_scaled, y_test)\n",
    "\n",
    "# Export RandomForest model\n",
    "rf_export = {\n",
    "    'model_type': 'RandomForestRegressor',\n",
    "    'n_estimators': int(simple_rf.n_estimators),\n",
    "    'feature_names': numerical_features_only,\n",
    "    'feature_count': len(numerical_features_only),\n",
    "    'n_features_in_': int(simple_rf.n_features_in_),\n",
    "    'n_outputs_': int(simple_rf.n_outputs_),\n",
    "    'performance': {\n",
    "        'r2_score': float(rf_score),\n",
    "        'training_samples': len(X_numerical)\n",
    "    },\n",
    "    'feature_statistics': convert_to_json_serializable({\n",
    "        'mean': X_numerical.mean().to_dict(),\n",
    "        'std': X_numerical.std().to_dict(),\n",
    "        'min': X_numerical.min().to_dict(),\n",
    "        'max': X_numerical.max().to_dict(),\n",
    "        'median': X_numerical.median().to_dict()\n",
    "    }),\n",
    "    'target_statistics': {\n",
    "        'mean': float(y_clean.mean()),\n",
    "        'std': float(y_clean.std()),\n",
    "        'min': float(y_clean.min()),\n",
    "        'max': float(y_clean.max()),\n",
    "        'median': float(y_clean.median())\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'y_skewness': float(y_skewness),\n",
    "        'log_transformed': bool(abs(y_skewness) > 1)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add feature importance\n",
    "if hasattr(simple_rf, 'feature_importances_'):\n",
    "    rf_export['feature_importances'] = {k: float(v) for k, v in zip(numerical_features_only, simple_rf.feature_importances_)}\n",
    "\n",
    "# Export LinearRegression model\n",
    "lr_export = {\n",
    "    'model_type': 'LinearRegression',\n",
    "    'feature_names': numerical_features_only,\n",
    "    'feature_count': len(numerical_features_only),\n",
    "    'coefficients': [float(x) for x in simple_lr.coef_],\n",
    "    'intercept': float(simple_lr.intercept_),\n",
    "    'performance': {\n",
    "        'r2_score': float(lr_score),\n",
    "        'training_samples': len(X_numerical)\n",
    "    },\n",
    "    'scaler_params': {\n",
    "        'mean': [float(x) for x in scaler.mean_],\n",
    "        'scale': [float(x) for x in scaler.scale_],\n",
    "        'var': [float(x) for x in scaler.var_]\n",
    "    },\n",
    "    'feature_statistics': rf_export['feature_statistics'],\n",
    "    'target_statistics': rf_export['target_statistics'],\n",
    "    'preprocessing': rf_export['preprocessing']\n",
    "}\n",
    "\n",
    "# Create sample predictions\n",
    "sample_data = []\n",
    "for _, row in X_test_numerical.head(5).iterrows():\n",
    "    sample_data.append({k: float(v) for k, v in row.to_dict().items()})\n",
    "\n",
    "sample_rf_preds = [float(x) for x in simple_rf.predict(X_test_numerical.head(5))]\n",
    "sample_lr_preds = [float(x) for x in simple_lr.predict(X_test_scaled[:5])]\n",
    "\n",
    "# Complete export with both models\n",
    "complete_export = {\n",
    "    'models': {\n",
    "        'random_forest': rf_export,\n",
    "        'linear_regression': lr_export\n",
    "    },\n",
    "    'sample_predictions': {\n",
    "        'input_data': sample_data,\n",
    "        'rf_predictions': sample_rf_preds,\n",
    "        'lr_predictions': sample_lr_preds\n",
    "    },\n",
    "    'metadata': {\n",
    "        'created_at': pd.Timestamp.now().isoformat(),\n",
    "        'feature_engineering_applied': True,\n",
    "        'text_data_available': True,\n",
    "        'original_feature_count': len(X_train.columns),\n",
    "        'simplified_feature_count': len(numerical_features_only)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON models\n",
    "with open('streamlit_simple_model.json', 'w') as f:\n",
    "    json.dump(rf_export, f, indent=2)\n",
    "\n",
    "with open('streamlit_linear_model.json', 'w') as f:\n",
    "    json.dump(lr_export, f, indent=2)\n",
    "\n",
    "with open('streamlit_complete_model.json', 'w') as f:\n",
    "    json.dump(complete_export, f, indent=2)\n",
    "\n",
    "# Save lightweight data for Streamlit\n",
    "sample_data_for_streamlit = {\n",
    "    'X_train_sample': X_train.head(100).to_dict('records'),\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'categorical_features': categorical_features,\n",
    "    'numerical_features': numerical_cols.tolist(),\n",
    "    'preprocessor_fitted': True,\n",
    "    'y_skewness': y_skewness,\n",
    "    'price_stats': {\n",
    "        'mean': df['price_clean'].mean(),\n",
    "        'std': df['price_clean'].std(),\n",
    "        'min': df['price_clean'].min(),\n",
    "        'max': df['price_clean'].max()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('model_data_for_streamlit.json', 'w') as f:\n",
    "    json.dump(sample_data_for_streamlit, f, indent=2)\n",
    "\n",
    "# Save preprocessor separately\n",
    "with open('preprocessor_simple.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f, protocol=4)\n",
    "\n",
    "# Save model state\n",
    "model_state = {\n",
    "    'model_type': 'voting_regressor_with_multimodal',\n",
    "    'tabular_models': ['RandomForest', 'GradientBoosting', 'ExtraTrees'],\n",
    "    'meta_learner': 'RandomForest',\n",
    "    'text_encoder': 'DistilBERT',\n",
    "    'feature_count': len(X_train.columns),\n",
    "    'training_samples': len(X_train),\n",
    "    'performance': {\n",
    "        'tabular_r2': float(test_score),\n",
    "        'multimodal_r2': float(multimodal_score)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('model_state.json', 'w') as f:\n",
    "    json.dump(model_state, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ JSON models created successfully!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 11. FINAL SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä TABULAR MODEL PERFORMANCE\")\n",
    "print(f\"   R¬≤ Score: {test_score:.3f} ({test_score*100:.1f}% accuracy)\")\n",
    "print(f\"   Cross-Validation: {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})\")\n",
    "print(f\"   MAE: ${mae:.2f}\")\n",
    "print()\n",
    "print(f\"üîó MULTIMODAL MODEL PERFORMANCE\")\n",
    "print(f\"   R¬≤ Score: {multimodal_score:.3f} ({multimodal_score*100:.1f}% accuracy)\")\n",
    "print(f\"   MAE: ${multimodal_mae:.2f}\")\n",
    "print()\n",
    "print(f\"üöÄ IMPROVEMENT\")\n",
    "print(f\"   R¬≤ Improvement: +{improvement:.1f}%\")\n",
    "print(f\"   MAE Improvement: +{mae_improvement:.1f}%\")\n",
    "print()\n",
    "print(f\"üìÅ FILES CREATED:\")\n",
    "print(f\"   ‚úÖ tabular_model_clean.pkl\")\n",
    "print(f\"   ‚úÖ multimodal_model_clean.pkl\") \n",
    "print(f\"   ‚úÖ preprocessor_clean.pkl\")\n",
    "print(f\"   ‚úÖ metadata_clean.pkl\")\n",
    "print(f\"   ‚úÖ streamlit_simple_model.json\")\n",
    "print(f\"   ‚úÖ streamlit_linear_model.json\")\n",
    "print(f\"   ‚úÖ streamlit_complete_model.json\")\n",
    "print(f\"   ‚úÖ model_data_for_streamlit.json\")\n",
    "print(f\"   ‚úÖ model_state.json\")\n",
    "print(f\"   ‚úÖ preprocessor_simple.pkl\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ Ready for Streamlit deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
